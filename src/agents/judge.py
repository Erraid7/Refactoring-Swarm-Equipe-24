"""
Agent Testeur - √âvalue la qualit√© du code et ex√©cute les tests
Utilise le judge_toolkit du Toolsmith
"""

from src.agents.base_agent import BaseAgent
from src.orchestration.state import RefactoringState, AgentStatus
from src.utils.logger import log_experiment, ActionType
from src.tools.judge_toolkit import (
    evaluate_code,
    format_test_feedback_for_fixer,
    compare_quality
)

class JudgeAgent(BaseAgent):
    """Agent qui teste et √©value la qualit√© du code"""
    
    def __init__(self):
        super().__init__(name="Judge", model="gemini-2.0-flash-exp")
    
    def execute(self, state: RefactoringState) -> RefactoringState:
        """√âvalue le code et ex√©cute les tests"""
        print(f"\n{'='*60}")
        print(f"‚öñÔ∏è  {self.name} Agent: Evaluating code quality")
        print(f"{'='*60}")
        
        state.current_agent = self.name
        state.agent_status = AgentStatus.RUNNING
        
        try:
            # √âtape 1: √âvaluer le code avec le toolkit
            print(f"üß™ Running tests and quality checks...")
            evaluation = evaluate_code(state.target_dir)
            
            # √âtape 2: Mettre √† jour l'√©tat
            state.test_results = {
                "passed": evaluation['passed'],
                "errors": evaluation['errors']
            }
            state.tests_passed = evaluation['passed']
            
            # Mettre √† jour le score Pylint actuel
            if 'pylint_score' in evaluation:
                state.pylint_score_current = evaluation['pylint_score']
            
            # Afficher les r√©sultats
            print(f"  ‚úÖ Evaluation complete")
            print(f"  üìä Pylint score: {state.pylint_score_current:.2f}/10")
            
            if evaluation['passed']:
                print(f"  ‚úÖ All tests passed!")
                
                # Comparer la qualit√©
                comparison = compare_quality(
                    state.pylint_score_initial,
                    state.pylint_score_current
                )
                print(f"  üìà Quality improvement: {comparison['improvement']:.2f} points ({comparison['percentage']:.1f}%)")
            else:
                print(f"  ‚ùå Tests failed: {len(evaluation['errors'])} errors")
                for i, error in enumerate(evaluation['errors'][:3], 1):
                    print(f"     {i}. {error[:100]}...")
                if len(evaluation['errors']) > 3:
                    print(f"     ... and {len(evaluation['errors'])-3} more errors")
                
                # Formater le feedback pour le Fixer
                feedback = format_test_feedback_for_fixer(evaluation)
                state.test_results['feedback'] = feedback
            
            # √âtape 3: Logger l'exp√©rience (OBLIGATOIRE)
            action_type = ActionType.DEBUG if not evaluation['passed'] else ActionType.ANALYSIS
            
            log_experiment(
                agent_name=self.name,
                model_used=self.model,
                action=action_type,
                details={
                    "input_prompt": f"Evaluate code quality and tests for iteration {state.current_iteration}",
                    "output_response": f"Tests: {'PASSED' if evaluation['passed'] else 'FAILED'}, Pylint: {state.pylint_score_current:.2f}",
                    "tests_passed": evaluation['passed'],
                    "pylint_score": state.pylint_score_current,
                    "test_errors_count": len(evaluation['errors']),
                    "iteration": state.current_iteration
                },
                status="SUCCESS" if evaluation['passed'] else "PARTIAL"
            )
            
            state.agent_status = AgentStatus.SUCCESS
            print(f"‚úÖ {self.name} Agent: Completed successfully\n")
            return state
            
        except Exception as e:
            print(f"‚ùå {self.name} Agent: Failed with error: {e}")
            state.agent_status = AgentStatus.FAILED
            state.error_message = str(e)
            
            # Logger l'erreur
            log_experiment(
                agent_name=self.name,
                model_used=self.model,
                action=ActionType.DEBUG,
                details={
                    "input_prompt": f"Evaluate {state.target_dir}",
                    "output_response": f"Error: {str(e)}",
                    "error": str(e)
                },
                status="FAILED"
            )
            
            return state