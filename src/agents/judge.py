"""
Agent Testeur - √âvalue la qualit√© du code et ex√©cute les tests
Utilise Gemini + judge_toolkit du Toolsmith
"""

from src.agents.base_agent import BaseAgent
from src.orchestration.state import RefactoringState, AgentStatus
from src.utils.logger import log_experiment, ActionType
from src.tools.judge_toolkit import (
    evaluate_code,
    format_test_feedback_for_fixer,
    compare_quality
)


class JudgeAgent(BaseAgent):
    """Agent qui teste et √©value la qualit√© du code"""
    
    def __init__(self):
        super().__init__(name="Judge", model="gemini-2.5-flash")
    
    def execute(self, state: RefactoringState) -> RefactoringState:
        """√âvalue le code et ex√©cute les tests"""
        print(f"\n{'='*60}")
        print(f"‚öñÔ∏è  {self.name}: Evaluating code quality")
        print(f"{'='*60}")
        
        state.current_agent = self.name
        state.agent_status = AgentStatus.RUNNING
        
        try:
            # √âtape 1: √âvaluer le code avec le toolkit
            print(f"üß™ Running tests and quality checks...")
            evaluation = evaluate_code(state.target_dir)
            
            # √âtape 2: Mettre √† jour l'√©tat
            state.test_results = {
                "passed": evaluation['passed'],
                "errors": evaluation['errors']
            }
            state.tests_passed = evaluation['passed']
            
            # Mettre √† jour le score Pylint actuel
            if 'pylint_score' in evaluation:
                state.pylint_score_current = evaluation['pylint_score']
            
            # Afficher les r√©sultats
            print(f"  ‚úÖ Evaluation complete")
            print(f"  üìä Pylint score: {state.pylint_score_current:.2f}/10 (was {state.pylint_score_initial:.2f})")
            
            if evaluation['passed']:
                print(f"  ‚úÖ All tests passed!")
                
                # Comparer la qualit√©
                if state.pylint_score_initial > 0:
                    comparison = compare_quality(
                        state.pylint_score_initial,
                        state.pylint_score_current
                    )
                    print(f"  üìà Quality improvement: {comparison['improvement']:.2f} points ({comparison['percentage']:.1f}%)")
                
                # G√©n√©rer un rapport de succ√®s avec Gemini
                success_prompt = self._build_success_report_prompt(state)
                success_report = self._call_llm(success_prompt, temperature=0.3)
                print(f"\nüìã Success Report:\n{success_report}\n")
                
            else:
                print(f"  ‚ùå Tests failed: {len(evaluation['errors'])} errors")
                
                # Afficher les premi√®res erreurs
                for i, error in enumerate(evaluation['errors'][:3], 1):
                    error_preview = error[:150].replace('\n', ' ')
                    print(f"     {i}. {error_preview}...")
                
                if len(evaluation['errors']) > 3:
                    print(f"     ... and {len(evaluation['errors'])-3} more errors")
                
                # Formater le feedback pour le Fixer
                feedback = format_test_feedback_for_fixer(evaluation)
                state.test_results['feedback'] = feedback
                
                print(f"\n  üìù Feedback prepared for next Fixer iteration")
            
            # √âtape 3: Logger l'exp√©rience (OBLIGATOIRE)
            action_type = ActionType.DEBUG if not evaluation['passed'] else ActionType.ANALYSIS
            
            log_experiment(
                agent_name=self.name,
                model_used=self.model,
                action=action_type,
                details={
                    "input_prompt": f"Evaluate code quality for iteration {state.current_iteration}",
                    "output_response": f"Tests: {'PASSED' if evaluation['passed'] else 'FAILED'}, Pylint: {state.pylint_score_current:.2f}",
                    "tests_passed": evaluation['passed'],
                    "pylint_score": state.pylint_score_current,
                    "pylint_improvement": state.pylint_score_current - state.pylint_score_initial,
                    "test_errors_count": len(evaluation['errors']),
                    "iteration": state.current_iteration
                },
                status="SUCCESS" if evaluation['passed'] else "PARTIAL"
            )
            
            state.agent_status = AgentStatus.SUCCESS
            print(f"‚úÖ {self.name}: Completed successfully\n")
            return state
            
        except Exception as e:
            print(f"‚ùå {self.name}: Failed with error: {e}")
            state.agent_status = AgentStatus.FAILED
            state.error_message = str(e)
            
            # Logger l'erreur
            log_experiment(
                agent_name=self.name,
                model_used=self.model,
                action=ActionType.DEBUG,
                details={
                    "input_prompt": f"Evaluate {state.target_dir}",
                    "output_response": f"Error: {str(e)}",
                    "error": str(e),
                    "iteration": state.current_iteration
                },
                status="FAILED"
            )
            
            return state
    
    def _build_success_report_prompt(self, state: RefactoringState) -> str:
        """Construit un prompt pour g√©n√©rer un rapport de succ√®s"""
        prompt = f"""You are a code quality analyst. The refactoring process has succeeded!

**Initial State:**
- Pylint Score: {state.pylint_score_initial:.2f}/10
- Files: {len(state.files_to_process)}

**Final State:**
- Pylint Score: {state.pylint_score_current:.2f}/10
- All tests passing: YES
- Iterations needed: {state.current_iteration}

**Refactoring Plan Applied:**
{chr(10).join(f'- {action}' for action in (state.audit_report.get('plan', []) if state.audit_report else []))}

Generate a brief success report (2-3 sentences) highlighting the improvements made.
"""
        return prompt